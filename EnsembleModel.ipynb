{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjbmsda/EnsembleModel/blob/main/EnsembleModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83mOyyMtXKeo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import resnet18, densenet121\n",
        "import torchaudio\n",
        "from torchaudio.datasets import LIBRISPEECH\n",
        "from torchaudio.transforms import MFCC\n",
        "from sklearn.metrics import accuracy_score, roc_curve\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 사용자 정의 PadTrim 함수\n",
        "def pad_trim(audio, target_length, pad_value=0):\n",
        "    \"\"\"Pad or trim the audio tensor to a fixed length.\"\"\"\n",
        "    length = audio.size(-1)  # 시간 축 길이\n",
        "    if length > target_length:  # Trim\n",
        "        audio = audio[..., :target_length]  # 마지막 차원을 트림\n",
        "    elif length < target_length:  # Pad\n",
        "        pad_amount = target_length - length\n",
        "        pad_shape = list(audio.shape[:-1]) + [pad_amount]  # 기존 차원 유지\n",
        "        padding = torch.full(pad_shape, pad_value, device=audio.device)\n",
        "        audio = torch.cat([audio, padding], dim=-1)  # 시간 축에 따라 패딩 추가\n",
        "    return audio\n",
        "\n",
        "# 입력 크기 조정 함수\n",
        "def pad_or_resize(audio, target_height=50, target_width=50):\n",
        "    \"\"\"오디오 데이터를 모델에 적합한 크기로 패딩 또는 리사이즈.\"\"\"\n",
        "    _, height, width = audio.shape\n",
        "    if height < target_height or width < target_width:\n",
        "        # 패딩 추가\n",
        "        pad_height = max(0, target_height - height)\n",
        "        pad_width = max(0, target_width - width)\n",
        "        audio = F.pad(audio, (0, pad_width, 0, pad_height))\n",
        "    elif height > target_height or width > target_width:\n",
        "        # 크기 리샘플링\n",
        "        audio = F.interpolate(audio.unsqueeze(0), size=(target_height, target_width), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
        "    return audio\n",
        "\n",
        "# LibriSpeechDataset 클래스\n",
        "class LibriSpeechDataset(Dataset):\n",
        "    def __init__(self, dataset, transform=None, target_length=None, label_to_index=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "        self.target_length = target_length\n",
        "        self.label_to_index = label_to_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio, sr, label, *_ = self.dataset[idx]\n",
        "        if self.transform:\n",
        "            audio = self.transform(audio)\n",
        "        if self.target_length:\n",
        "            audio = pad_trim(audio, self.target_length)\n",
        "        # 크기 조정\n",
        "        audio = pad_or_resize(audio, target_height=50, target_width=50)\n",
        "        label_idx = self.label_to_index[label]\n",
        "        label_tensor = torch.tensor(label_idx, dtype=torch.long)\n",
        "        return audio, label_tensor\n",
        "\n",
        "# MFCC 변환\n",
        "mfcc_transform = MFCC(sample_rate=16000, n_mfcc=13, log_mels=True)\n",
        "\n",
        "# 데이터 디렉토리 생성\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "\n",
        "# LibriSpeech 데이터셋 다운로드 및 로드\n",
        "train_data = LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=True)\n",
        "test_data = LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)\n",
        "\n",
        "# 라벨 매핑 생성\n",
        "speaker_ids = set()\n",
        "for _, _, label, *_ in train_data:\n",
        "    speaker_ids.add(label)\n",
        "label_to_index = {label: idx for idx, label in enumerate(sorted(speaker_ids))}\n",
        "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
        "\n",
        "# 최대 길이를 계산\n",
        "max_length = 0\n",
        "for audio, _, _, *_ in train_data:\n",
        "    audio = audio[:, :80000]  # 긴 오디오를 5초로 트림\n",
        "    mfcc = mfcc_transform(audio)\n",
        "    max_length = max(max_length, mfcc.shape[-1])\n",
        "target_length = max(max_length, 50)  # 최소 길이를 50으로 보장\n",
        "\n",
        "# 데이터셋 생성\n",
        "train_dataset = LibriSpeechDataset(\n",
        "    train_data,\n",
        "    transform=mfcc_transform,\n",
        "    target_length=target_length,\n",
        "    label_to_index=label_to_index\n",
        ")\n",
        "test_dataset = LibriSpeechDataset(\n",
        "    test_data,\n",
        "    transform=mfcc_transform,\n",
        "    target_length=target_length,\n",
        "    label_to_index=label_to_index\n",
        ")\n",
        "\n",
        "# DataLoader에서 배치 크기 축소\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
        "\n",
        "# ResNet 및 DenseNet 정의\n",
        "class ResNetModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNetModel, self).__init__()\n",
        "        self.resnet = resnet18(pretrained=False, num_classes=num_classes)\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.resnet.maxpool = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "class DenseNetModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DenseNetModel, self).__init__()\n",
        "        self.densenet = densenet121(pretrained=False)\n",
        "        self.densenet.features.conv0 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.densenet.features.pool0 = nn.Identity()\n",
        "        self.densenet.classifier = nn.Linear(self.densenet.classifier.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.densenet(x)\n",
        "\n",
        "# GPU 메모리 정리\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 모델 초기화\n",
        "num_classes = len(speaker_ids)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet_model = ResNetModel(num_classes).to(device)\n",
        "densenet_model = DenseNetModel(num_classes).to(device)\n",
        "\n",
        "# 손실 함수 및 옵티마이저\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_resnet = optim.Adam(resnet_model.parameters(), lr=0.001)\n",
        "optimizer_densenet = optim.Adam(densenet_model.parameters(), lr=0.001)\n",
        "\n",
        "# Mixed Precision 적용한 학습 함수\n",
        "def train(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# 평가 함수\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            preds.extend(predicted.cpu().numpy())\n",
        "            targets.extend(labels.cpu().numpy())\n",
        "    acc = accuracy_score(targets, preds)\n",
        "    return acc, preds, targets\n",
        "\n",
        "# 학습 루프\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    resnet_loss = train(resnet_model, train_loader, criterion, optimizer_resnet)\n",
        "    densenet_loss = train(densenet_model, train_loader, criterion, optimizer_densenet)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    print(f\"ResNet Loss: {resnet_loss:.4f}\")\n",
        "    print(f\"DenseNet Loss: {densenet_loss:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPxzcUIJgAKXsPZj55RSxz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}