{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjbmsda/EnsembleModel/blob/main/EnsembleModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83mOyyMtXKeo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.models import resnet152, densenet121\n",
        "import torchaudio\n",
        "from torchaudio.datasets import LIBRISPEECH\n",
        "from torchaudio.transforms import MFCC\n",
        "from sklearn.metrics import accuracy_score, roc_curve\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 사용자 정의 PadTrim 함수\n",
        "def pad_trim(audio, target_length, pad_value=0):\n",
        "    \"\"\"Pad or trim the audio tensor to a fixed length.\"\"\"\n",
        "    length = audio.size(-1)  # 시간 축 길이\n",
        "    if length > target_length:  # Trim\n",
        "        audio = audio[..., :target_length]  # 마지막 차원을 트림\n",
        "    elif length < target_length:  # Pad\n",
        "        pad_amount = target_length - length\n",
        "        pad_shape = list(audio.shape[:-1]) + [pad_amount]  # 기존 차원 유지\n",
        "        padding = torch.full(pad_shape, pad_value, device=audio.device)\n",
        "        audio = torch.cat([audio, padding], dim=-1)  # 시간 축에 따라 패딩 추가\n",
        "    return audio\n",
        "\n",
        "# 입력 크기 조정 함수\n",
        "def pad_or_resize(audio, target_height=50, target_width=50):\n",
        "    \"\"\"오디오 데이터를 모델에 적합한 크기로 패딩 또는 리사이즈.\"\"\"\n",
        "    _, height, width = audio.shape\n",
        "    if height < target_height or width < target_width:\n",
        "        # 패딩 추가\n",
        "        pad_height = max(0, target_height - height)\n",
        "        pad_width = max(0, target_width - width)\n",
        "        audio = F.pad(audio, (0, pad_width, 0, pad_height))\n",
        "    elif height > target_height or width > target_width:\n",
        "        # 크기 리샘플링\n",
        "        audio = F.interpolate(audio.unsqueeze(0), size=(target_height, target_width), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
        "    return audio\n",
        "\n",
        "# LibriSpeechDataset 클래스\n",
        "class LibriSpeechDataset(Dataset):\n",
        "    def __init__(self, dataset, transform=None, target_length=None, label_to_index=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "        self.target_length = target_length\n",
        "        self.label_to_index = label_to_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio, sr, label, *_ = self.dataset[idx]\n",
        "        if self.transform:\n",
        "            audio = self.transform(audio)\n",
        "        if self.target_length:\n",
        "            audio = pad_trim(audio, self.target_length)\n",
        "        # 크기 조정\n",
        "        audio = pad_or_resize(audio, target_height=50, target_width=50)\n",
        "        label_idx = self.label_to_index[label]\n",
        "        label_tensor = torch.tensor(label_idx, dtype=torch.long)\n",
        "        return audio, label_tensor\n",
        "\n",
        "# MFCC 변환\n",
        "mfcc_transform = MFCC(sample_rate=16000, n_mfcc=13, log_mels=True)\n",
        "\n",
        "# 데이터 디렉토리 생성\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "\n",
        "# LibriSpeech 데이터셋 다운로드 및 로드\n",
        "train_data = LIBRISPEECH(\"./data\", url=\"train-clean-100\", download=True)\n",
        "test_data = LIBRISPEECH(\"./data\", url=\"test-clean\", download=True)\n",
        "\n",
        "# 라벨 매핑 생성\n",
        "speaker_ids = set()\n",
        "for _, _, label, *_ in train_data:\n",
        "    speaker_ids.add(label)\n",
        "label_to_index = {label: idx for idx, label in enumerate(sorted(speaker_ids))}\n",
        "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
        "\n",
        "# 최대 길이를 계산\n",
        "max_length = 0\n",
        "for audio, _, _, *_ in train_data:\n",
        "    audio = audio[:, :80000]  # 긴 오디오를 5초로 트림\n",
        "    mfcc = mfcc_transform(audio)\n",
        "    max_length = max(max_length, mfcc.shape[-1])\n",
        "target_length = max(max_length, 50)  # 최소 길이를 50으로 보장\n",
        "\n",
        "# 데이터셋 생성\n",
        "train_dataset = LibriSpeechDataset(\n",
        "    train_data,\n",
        "    transform=mfcc_transform,\n",
        "    target_length=target_length,\n",
        "    label_to_index=label_to_index\n",
        ")\n",
        "test_dataset = LibriSpeechDataset(\n",
        "    test_data,\n",
        "    transform=mfcc_transform,\n",
        "    target_length=target_length,\n",
        "    label_to_index=label_to_index\n",
        ")\n",
        "\n",
        "# DataLoader에서 num_workers를 0으로 설정\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
        "\n",
        "# ResNet 및 DenseNet 모델 정의\n",
        "class ResNetModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNetModel, self).__init__()\n",
        "        self.resnet = resnet152(pretrained=False, num_classes=num_classes)\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)  # 스트라이드와 커널 크기 조정\n",
        "        self.resnet.maxpool = nn.Identity()  # MaxPooling 제거 또는 대체\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "class DenseNetModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DenseNetModel, self).__init__()\n",
        "        self.densenet = densenet121(pretrained=False)\n",
        "        self.densenet.features.conv0 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)  # 스트라이드와 커널 크기 조정\n",
        "        self.densenet.features.pool0 = nn.Identity()  # 첫 번째 Pooling 제거 또는 대체\n",
        "        self.densenet.classifier = nn.Linear(self.densenet.classifier.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.densenet(x)\n",
        "\n",
        "class EnsembleModel(nn.Module):\n",
        "    def __init__(self, resnet, densenet):\n",
        "        super(EnsembleModel, self).__init__()\n",
        "        self.resnet = resnet\n",
        "        self.densenet = densenet\n",
        "\n",
        "    def forward(self, x):\n",
        "        resnet_out = self.resnet(x)\n",
        "        densenet_out = self.densenet(x)\n",
        "        return (resnet_out + densenet_out) / 2\n",
        "\n",
        "# 모델 초기화\n",
        "num_classes = len(speaker_ids)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "resnet_model = ResNetModel(num_classes).to(device)\n",
        "densenet_model = DenseNetModel(num_classes).to(device)\n",
        "ensemble_model = EnsembleModel(resnet_model, densenet_model).to(device)\n",
        "\n",
        "# 손실 함수 및 옵티마이저\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_resnet = optim.Adam(resnet_model.parameters(), lr=0.001)\n",
        "optimizer_densenet = optim.Adam(densenet_model.parameters(), lr=0.001)\n",
        "optimizer_ensemble = optim.Adam(ensemble_model.parameters(), lr=0.001)\n",
        "\n",
        "# 학습 함수\n",
        "def train(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# 평가 함수\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            preds.extend(predicted.cpu().numpy())\n",
        "            targets.extend(labels.cpu().numpy())\n",
        "    acc = accuracy_score(targets, preds)\n",
        "    return acc, preds, targets\n",
        "\n",
        "# EER 계산 함수\n",
        "def calculate_eer(targets, preds):\n",
        "    fpr, tpr, thresholds = roc_curve(targets, preds, pos_label=1)\n",
        "    fnr = 1 - tpr\n",
        "    eer_threshold = thresholds[np.nanargmin(np.absolute(fnr - fpr))]\n",
        "    eer = fpr[np.nanargmin(np.absolute(fnr - fpr))]\n",
        "    return eer, eer_threshold\n",
        "\n",
        "# 학습 및 평가 루프\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    resnet_loss = train(resnet_model, train_loader, criterion, optimizer_resnet)\n",
        "    densenet_loss = train(densenet_model, train_loader, criterion, optimizer_densenet)\n",
        "    ensemble_loss = train(ensemble_model, train_loader, criterion, optimizer_ensemble)\n",
        "\n",
        "    resnet_acc, resnet_preds, resnet_targets = evaluate(resnet_model, test_loader)\n",
        "    densenet_acc, densenet_preds, densenet_targets = evaluate(densenet_model, test_loader)\n",
        "    ensemble_acc, ensemble_preds, ensemble_targets = evaluate(ensemble_model, test_loader)\n",
        "\n",
        "    resnet_eer, _ = calculate_eer(resnet_targets, resnet_preds)\n",
        "    densenet_eer, _ = calculate_eer(densenet_targets, densenet_preds)\n",
        "    ensemble_eer, _ = calculate_eer(ensemble_targets, ensemble_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    print(f\"ResNet - Loss: {resnet_loss:.4f}, Accuracy: {resnet_acc:.4f}, EER: {resnet_eer:.4f}\")\n",
        "    print(f\"DenseNet - Loss: {densenet_loss:.4f}, Accuracy: {densenet_acc:.4f}, EER: {densenet_eer:.4f}\")\n",
        "    print(f\"Ensemble - Loss: {ensemble_loss:.4f}, Accuracy: {ensemble_acc:.4f}, EER: {ensemble_eer:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+ro5cGoFbpee/5016EJZc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}